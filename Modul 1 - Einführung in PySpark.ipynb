{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586516be-bef2-4de5-b8f3-929eed067caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modul1: Einführung in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5efbdb2c-b7e7-4f08-8bdc-1a05c6d6e8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1. Einführung in Apache Spark\n",
    "Apache Spark ist ein Framework für **verteilte Datenverarbeitung**, das auf großen Clustern effizient arbeitet.\n",
    "\n",
    "Kernmerkmale:\n",
    "\n",
    "- **In-Memory-Verarbeitung:** Spark kann Daten im RAM halten, was die Verarbeitung im Vergleich zu diskbasierten Systemen stark beschleunigt.\n",
    "- **Skalierbarkeit:** Spark läuft auf einem einzelnen Rechner, aber auch auf Hunderten von Nodes in einem Cluster.\n",
    "- **Vielseitigkeit:** Spark unterstützt Batch-Processing, Streaming, SQL-Abfragen, Machine Learning (MLlib) und Graphverarbeitung (GraphX).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a643e2-0c4f-48ab-8898-af900a83eb6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.1. Vorteile von Spark gegenüber klassischen Ansätzen\n",
    "\n",
    "| Vorteil                     | Beschreibung |\n",
    "|-------------------------------|-------------|\n",
    "| Geschwindigkeit               | Bis zu 100x schneller als klassische MapReduce-Ansätze, dank In-Memory-Verarbeitung |\n",
    "| Skalierbarkeit                 | Verarbeitung von Terabytes oder Petabytes an Daten über Cluster hinweg |\n",
    "| Flexibilität                   | Unterstützt unterschiedliche Datenformate (CSV, JSON, Parquet, Delta) und APIs (Python, Scala, Java, R) |\n",
    "| Einheitliche Plattform         | Ein Framework für ETL, Analytics, Machine Learning und Streaming |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f17be15-7fa5-421d-bd98-dec3b609109a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.2. Cluster-Architektur von Spark\n",
    "\n",
    "Spark arbeitet verteilt auf einem Cluster. Die wichtigsten Komponenten:\n",
    "\n",
    "- **Cluster Manager**:\n",
    "  - Zuteilung von Ressourcen im Cluster (z. B. Standalone, YARN, Databricks-eigener Manager)\n",
    "- **Driver**: \n",
    "  - Koordiniert die Berechnungen\n",
    "  - Plant Tasks und verwaltet die SparkContext / SparkSession\n",
    "- **Worker Nodes**:\n",
    "  - Maschinen im Cluster, auf denen Executor-Prozesse laufen\n",
    "- **Executors**:\n",
    "  - Führen die Tasks auf den Worker Nodes aus\n",
    "  - Jeder Executor verwaltet einen Teil des Arbeitsspeichers und der CPU-Kerne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e3d6de7-2f12-42a1-8027-e553ecea7374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.3. Lazy Evaluation\n",
    "\n",
    "- Spark führt **Transformationen** (z. B. `filter`, `select`) **nicht sofort** aus.\n",
    "- Erst eine **Action** (z. B. `count()`, `show()`) löst die Berechnung aus.\n",
    "- Vorteil: Optimierung durch Spark (z. B. Catalyst Optimizer, Pipeline-Fusion).\n",
    "\n",
    "Beispiel:  \n",
    "\n",
    "```python\n",
    "df_filtered = df.filter(df.Age > 30).select(\"Name\")\n",
    "# Keine Berechnung bisher\n",
    "df_filtered.show()  # Jetzt werden die Daten berechnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf7cc13-3aac-4a32-a265-04ec150d5d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2. PySpark im Überblick\n",
    "\n",
    "PySpark ist die **Python-Schnittstelle** zu Apache Spark.  \n",
    "Damit können Python-Entwickler Spark-Funktionen nutzen, ohne Scala oder Java lernen zu müssen.\n",
    "\n",
    "**Kernideen von PySpark:**\n",
    "- Zugriff auf Spark Core, Spark SQL, MLlib, GraphX und Streaming über Python\n",
    "- Verteilte Datenverarbeitung auf Clustern\n",
    "- Lazy Evaluation: Transformationen werden erst ausgeführt, wenn eine Action aufgerufen wird\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3918fedd-a045-4f04-96f6-911e9f0ad24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.3. Komponenten von Apache Spark\n",
    "\n",
    "| Komponente | Beschreibung |\n",
    "|------------|-------------|\n",
    "| Spark Core | Kern von Spark, verantwortlich für Task Scheduling, Speicherverwaltung und Cluster-Management |\n",
    "| Spark SQL  | Datenanalyse mit SQL-ähnlichen Abfragen, unterstützt DataFrames und Datasets |\n",
    "| MLlib      | Machine Learning Bibliothek für verteilte ML-Algorithmen |\n",
    "| Spark Streaming | Verarbeitung von Echtzeit-Datenströmen |\n",
    "| GraphX     | Graph-Processing API |\n",
    "\n",
    "PySpark gibt uns Zugang zu all diesen Komponenten direkt aus Python heraus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c916c2-5172-420d-9a08-c6a8406ccd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.4. DataFrames und Transformationen\n",
    "\n",
    "In PySpark arbeiten wir meist mit **DataFrames**, die verteilt auf dem Cluster liegen.\n",
    "\n",
    "Wichtige Konzepte:\n",
    "\n",
    "- **Transformationen**: z. B. `select()`, `filter()`, `groupBy()` – werden **lazy** ausgeführt\n",
    "- **Actions**: z. B. `show()`, `count()`, `collect()` – lösen die tatsächliche Berechnung aus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0aa98c-df11-4d42-b6b9-b40ebf2c93b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Beispiel\n",
    "\n",
    "# 1. SparkSession erstellen\n",
    "\n",
    "#INFO: Databricks erstellt eine spark Session automatisch, manuelle Erstellung in der Praxis nur notwendig fuer spezielle Settings wie hier am Beispiel gezeigt\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Einführung Apache Spark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Ein DataFrame erstellen\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Alter\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# 3. Transformation (Lazy)\n",
    "df_filtered = df.filter(df.Alter > 30).select(\"Name\")\n",
    "\n",
    "# 4. Action\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11f30507-22bd-4c29-b0d0-aa3a07ee5b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.5. Narrow vs. Wide Transformations\n",
    "\n",
    "Spark-Transformationen lassen sich in zwei Typen einteilen:\n",
    "\n",
    "#### **Narrow Transformations**\n",
    "- Jede Partition des Input-RDDs/DFs wird nur auf **eine Partition des Outputs** gemappt.\n",
    "- Kein Daten-Shuffling zwischen Nodes.\n",
    "- Beispiele: `map()`, `filter()`, `select()`\n",
    "- Vorteil: Sehr effizient, da keine Daten über das Netzwerk bewegt werden müssen.\n",
    "\n",
    "#### **Wide Transformations**\n",
    "- Eine Partition des Outputs hängt von **mehreren Partitionen des Inputs** ab.\n",
    "- Spark muss Daten zwischen Nodes verschieben → **Shuffle**.\n",
    "- Beispiele: `groupByKey()`, `reduceByKey()`, `join()`\n",
    "- Vorteil: notwendig für Aggregationen und komplexe Operationen, aber teurer in Performance.\n",
    "\n",
    "#### Shuffle in Spark\n",
    "\n",
    "**Shuffle** = das Umverteilen von Daten über Nodes im Cluster.  \n",
    "- Entsteht bei **Wide Transformations**\n",
    "- Spark schreibt Partitionen auf Disk oder Netzwerk, sendet sie an andere Executor-Nodes\n",
    "- Teuer: Netzwerktransfer + Disk I/O + Sortierung\n",
    "\n",
    "**Warum ist Shuffle wichtig zu verstehen?**\n",
    "- Vermeidet Überraschungen bei Performanceproblemen\n",
    "- Hilft bei der Optimierung von Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da383a6-793b-49a3-9ff8-f728af6f2031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Praxisbeispiel: Narrow vs. Wide Transformationen\n",
    "\n",
    "data = [(\"Alice\", \"Math\", 85),\n",
    "        (\"Bob\", \"Math\", 90),\n",
    "        (\"Alice\", \"Physics\", 95),\n",
    "        (\"Bob\", \"Physics\", 80)]\n",
    "columns = [\"Name\", \"Fach\", \"Punkte\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Narrow Transformation (filter)\n",
    "df_filtered = df.filter(df.Punkte > 85)\n",
    "df_filtered.show()\n",
    "\n",
    "# Wide Transformation (groupBy)\n",
    "df_grouped = df.groupBy(\"Name\").sum(\"Punkte\")\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "594c4cb6-878b-4f84-ba03-8e64355a2517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.6. DataFrames vs. Spark SQL\n",
    "\n",
    "In Spark gibt es zwei Hauptmethoden, Daten zu verarbeiten: \n",
    "* **DataFrames** (API-basiert) und \n",
    "* **Spark SQL** (SQL-ähnliche Abfragen)\n",
    "\n",
    "Obwohl beide auf der gleichen Engine laufen und ähnliche Optimierungen nutzen, unterscheiden sie sich in Arbeitsweise, Syntax und Flexibilität.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ce1fc6-e9ca-4d01-b77e-a60b0a01562b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.6.1. DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6104f5a4-9596-47a7-a9fc-1ec1c2f056ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#**API-basiert**: Python-, Scala-, oder R-Methoden  \n",
    "#Unterstützt Transformationen und Aktionen direkt im Code  \n",
    "#Vorteil: Integration mit komplexer Logik, Bedingungen, User-Defined Functions (UDFs)  \n",
    "\n",
    "#Beispiel:\n",
    "# DataFrame erstellen\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Alter\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Transformation: Filter + Select\n",
    "df_filtered = df.filter(df.Alter > 30).select(\"Name\")\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c81673-45b0-4b2a-9f81-ee96a5a422c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.6.2. SQL\n",
    "\n",
    "- **SQL-basiert**: Daten mit bekannten SQL-Statements abfragen  \n",
    "- Vorteil: Leicht verständlich für Analysten, Data Engineers und SQL-Profis  \n",
    "- Transformationen werden ebenfalls **lazy** geplant und optimiert durch den **Catalyst Optimizer**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06872af2-1221-4957-bb0a-e028f975ec43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Beispiel:\n",
    "\n",
    "# DataFrame als temporäre View registrieren\n",
    "df.createOrReplaceTempView(\"personen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5431cde2-8095-4288-bb3b-d50707ccd5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from personen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe6a9f3-ed1a-4668-a078-d3a25e8fe941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.sql(\"select * from personen\")\n",
    "new_df.show()\n",
    "\n",
    "new_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"personen_tabelle\")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS neues_schema\")\n",
    "\n",
    "new_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"neues_schema.personen_tabelle\")\n",
    "\n",
    "#spark.sql(\"DROP SCHEMA neues_schema CASCADE\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6346898741128537,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Modul 1 - Einführung in PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
