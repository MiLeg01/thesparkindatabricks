{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c637ceae-a587-4a11-beb6-62cf1712f5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Panda Databricks Notebook: Structured Streaming Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717596fe-475d-4dd1-9709-80f055ed54ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Part 0: Workshop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17044170-64c7-45d4-bf6f-523bda87e83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a subfolder for the structured streaming work\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_dir = \"/\".join(notebook_path.split(\"/\")[:-1])\n",
    "base_path = f\"/Workspace{notebook_dir}\"\n",
    "\n",
    "data_path = f\"{base_path}/data\"\n",
    "checkpoint_path = f\"{data_path}/checkpoints\"\n",
    "input_data_json_path = f\"{data_path}/input_data.json\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bebcda5-132b-406a-95b5-e8a22699ae9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp, sha1\n",
    "\n",
    "# Data contents and Delivery\n",
    "initial_data_delivery = (\n",
    "    [(\"Software Lizenzen\", \"2023-01-01\", 100), \n",
    "     (\"Betriebskosten\", \"2023-01-02\", 200), \n",
    "     (\"Personalkosten\", \"2023-01-03\", 300)\n",
    "    ], [\"name\", \"datum\", \"wert\"]\n",
    ")\n",
    "\n",
    "follow_up_data_delivery = (\n",
    "    [(\"Zusatzkosten\", \"2023-01-04\", 400),\n",
    "     (\"Zusaetzliche Zusatzkosten\", \"2023-01-05\", 500)\n",
    "    ], [\"name\", \"datum\", \"wert\"]\n",
    ")\n",
    "\n",
    "def write_data_as_json(data_delivery, input_data_json_path):\n",
    "    df = spark.createDataFrame(data_delivery[0], schema=data_delivery[1])\n",
    "    df.write.mode(\"overwrite\").json(f\"{input_data_json_path}\")\n",
    "    print(f\"Data written to {input_data_json_path}\")\n",
    "\n",
    "def append_data_as_json(data_delivery, input_data_json_path):\n",
    "    df = spark.createDataFrame(data_delivery[0], schema=data_delivery[1])\n",
    "    df.write.mode(\"append\").json(f\"{input_data_json_path}\")\n",
    "    print(f\"Data appended to {input_data_json_path}\")\n",
    "\n",
    "#Transformation function\n",
    "def panda_ws_transform(df):\n",
    "    return (df\n",
    "          .withColumn(\"steuer\", df.wert * 0.1)\n",
    "          .withColumn(\"datum\", to_timestamp(df.datum, \"yyyy-MM-dd\"))\n",
    "          .withColumn(\"hash_key\", sha1(df.name))\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03134954-e613-4e86-ba05-2cfd84ac662b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 1: Basic Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b37b8fe-eac9-4241-8c27-b7467e834848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_data_as_json(initial_data_delivery, input_data_json_path)\n",
    "output_table_path = f\"{data_path}/simple_table_write\"\n",
    "dbutils.fs.rm(output_table_path, recurse=True)\n",
    "\n",
    "# Step 1: Read data\n",
    "input_df = spark.read.format(\"delta\").json(input_data_json_path)\n",
    "print(\"Incoming data:\")\n",
    "input_df.show()\n",
    "\n",
    "# Step 2: Transform\n",
    "transformed_df = panda_ws_transform(input_df)\n",
    "\n",
    "# Step 3: Write Output\n",
    "transformed_df.write.mode(\"append\").parquet(output_table_path)\n",
    "\n",
    "print(f\"Data written to {output_table_path}\")\n",
    "display(spark.read.parquet(output_table_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370049b7-dbbb-4866-9225-6facd2d24451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Part 2: Basic ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c278a672-cfc9-4d3d-a31a-4842f6e5d8df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_data_as_json(initial_data_delivery, input_data_json_path)\n",
    "output_table_path = f\"{data_path}/simple_table_write\"\n",
    "dbutils.fs.rm(output_table_path, recurse=True)\n",
    "\n",
    "for write_iteration in [\"First write\", \"Second write\", \"Third Update\"]:\n",
    "    print(f\"\\n{write_iteration}:\")\n",
    "    # Step 1: Read data\n",
    "    input_df = spark.read.format(\"delta\").json(input_data_json_path)\n",
    "    print(\"Incoming data:\")\n",
    "    input_df.show()\n",
    "\n",
    "    # Step 2: Transform\n",
    "    transformed_df = panda_ws_transform(input_df)\n",
    "\n",
    "    # Step 3: Write Output\n",
    "    transformed_df.write.mode(\"append\").parquet(output_table_path)\n",
    "\n",
    "    print(f\"Data written to {output_table_path}\")\n",
    "    display(spark.read.parquet(output_table_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e281ee4-80a9-4a34-96a6-976afc962aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Part 3: Basic Merge Into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f954d9-5190-494a-b304-d57993901367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "#Write functions\n",
    "def ensure_table_exists(df, path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "    except Exception:\n",
    "        print(\"table does not exist, creating empty table\")\n",
    "        spark.createDataFrame([], df.schema).write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "        \n",
    "def panda_ws_write(df, batch_id, target_table_path, throwError = 0):\n",
    "    key_column = \"hash_key\"\n",
    "    merge_condition = f\"t.{key_column} = s.{key_column}\"\n",
    "    delta_table = DeltaTable.forPath(spark, target_table_path)\n",
    "    (\n",
    "        delta_table.alias(\"t\")\n",
    "        .merge(df.alias(\"s\"), merge_condition)\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    # Artificial error for test\n",
    "    if throwError == 1:\n",
    "        raise Exception(\"Artificial error for testing error handling in foreachBatch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb768915-4957-4687-ac1c-7f12115d26af",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"steuer\":146,\"wert\":155},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754045742067}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_data_as_json(initial_data_delivery, input_data_json_path)\n",
    "output_table_path = f\"{data_path}/simple_merge_write\"\n",
    "dbutils.fs.rm(output_table_path, recurse = True)\n",
    "\n",
    "for write_iteration in [\"First write\", \"Second write\", \"Third Update\"]:\n",
    "    print(f\"\\n{write_iteration}:\")\n",
    "    # Step 1: Read data as table\n",
    "    input_df = spark.read.json(input_data_json_path)\n",
    "    print(\"incoming data:\")\n",
    "    input_df.show()\n",
    "\n",
    "    # Step 2: Transform\n",
    "    transformed_df = panda_ws_transform(input_df)\n",
    "\n",
    "    # Step 3: Write Output\n",
    "    ensure_table_exists(transformed_df, output_table_path)\n",
    "    panda_ws_write(transformed_df, 0, output_table_path)\n",
    "    print(f\"data written to {output_table_path}\")\n",
    "    display(spark.read.format('delta').load(output_table_path))\n",
    "\n",
    "    # Step 4: Repeat write, update source at penultimate iteration\n",
    "    if write_iteration == \"Second write\":\n",
    "        append_data_as_json(follow_up_data_delivery, input_data_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e428d584-5d27-44d3-9d3f-a3fc7073aa87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 4: Structured Streaming ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11de1957-4565-46a0-8ec8-8842c0357e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_streaming_table(output_table_path, output_checkpoint_path):\n",
    "    dbutils.fs.rm(output_table_path, recurse = True)\n",
    "    dbutils.fs.rm(output_checkpoint_path, recurse = True)\n",
    "\n",
    "def get_schema_from_json(input_data_path):\n",
    "    return spark.read.json(input_data_json_path).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9d0786-b8dd-4b57-8e87-927f61100ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_data_as_json(initial_data_delivery, input_data_json_path)\n",
    "output_table_path = f\"{data_path}/merge_stream\"\n",
    "output_checkpoint_path = output_table_path.replace(data_path, checkpoint_path)\n",
    "delete_streaming_table(output_table_path, output_checkpoint_path)\n",
    "\n",
    "for write_iteration in [\"First write\", \"Second write\", \"Third Update\"]:\n",
    "    print(f\"\\n{write_iteration}:\")\n",
    "\n",
    "    # Step 1: Stream Read\n",
    "    streaming_df = spark.readStream.schema(get_schema_from_json(input_data_json_path)).json(input_data_json_path)\n",
    "    #compare with input_df = spark.read.json(input_data_json_path)\n",
    "\n",
    "    # Step 2: Stream Transform\n",
    "    transformed_df = panda_ws_transform(streaming_df)\n",
    "\n",
    "    # # Any attempts to reference source data before writestream.start() throws an error:\n",
    "    # transformed_df.show()\n",
    "\n",
    "    # Step 3: Stream Write with foreachBatch\n",
    "    ensure_table_exists(transformed_df, output_table_path)\n",
    "    write_operation = (\n",
    "        transformed_df\n",
    "        .writeStream\n",
    "        .queryName(\"panda_stream_demo\")\n",
    "        .option(\"checkpointLocation\", output_checkpoint_path)\n",
    "        .trigger(availableNow=True)\n",
    "        .foreachBatch(lambda df, batch_id: panda_ws_write(df, batch_id, output_table_path)\n",
    "        ).start().awaitTermination()\n",
    "    )\n",
    "\n",
    "    print(f\"data written to {output_table_path}\")\n",
    "    display(spark.read.format('delta').load(output_table_path))\n",
    "\n",
    "    # Step 4: Repeat write, update source at penultimate iteration\n",
    "    if write_iteration == \"Second write\":\n",
    "        append_data_as_json(follow_up_data_delivery, input_data_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b95c3e-2460-4a0f-b590-b3a55914a913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Part 5: Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b62709-c064-4bf4-899e-4c0afabba88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_table_path = f\"{data_path}/merge_stream\"\n",
    "output_checkpoint_path = output_table_path.replace(data_path, checkpoint_path)\n",
    "delete_streaming_table(output_table_path, output_checkpoint_path) #consider not initializing the streaming table and then running the code, just to check\n",
    "\n",
    "# Step 1: Stream Read\n",
    "streaming_df = spark.readStream.schema(get_schema_from_json(input_data_json_path)).json(input_data_json_path)\n",
    "\n",
    "# Step 2: Stream Transform\n",
    "transformed_df = panda_ws_transform(streaming_df)\n",
    "\n",
    "# Step 3: Stream Write with foreachBatch\n",
    "ensure_table_exists(transformed_df, output_table_path)\n",
    "write_operation = (\n",
    "    transformed_df\n",
    "    .writeStream\n",
    "    .queryName(\"panda_stream_demo\")\n",
    "    .option(\"checkpointLocation\", output_checkpoint_path)\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(lambda df, batch_id: panda_ws_write(df, batch_id, output_table_path, throwError = 1) #throwError has/should be set to 1, to examine error handling\n",
    "    ).start().awaitTermination()\n",
    ")\n",
    "display(spark.read.format('delta').load(output_table_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Panda Structured streaming WS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
