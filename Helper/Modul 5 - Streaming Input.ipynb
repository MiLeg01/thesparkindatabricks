{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b228fb3f-2c89-4f7f-86fc-d06f130378e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBFS Pfad\n",
    "DATA_PATH = \"workspace.default.yellow_tripdata_2025_01\" # \"/FileStore/tables/yellow_tripdata_2025_01-1.parquet\"\n",
    "LOOKUP_PATH = \"workspace.default.df_lookup\"\n",
    "\n",
    "# DataFrame laden\n",
    "df_taxi = spark.read.table(DATA_PATH)\n",
    "df_lookup = spark.read.table(LOOKUP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c71e97-a558-413b-af44-89f29cda68c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Zielvolume\n",
    "streaming_input_volume = \"/Volumes/workspace/default/taxi_volume/\"\n",
    "\n",
    "df_taxi_small = df_taxi.take(10000)\n",
    "\n",
    "# Iterator über alle Rows\n",
    "for i, row in enumerate(df_taxi_small.collect()):\n",
    "    row_dict = row.asDict()\n",
    "    \n",
    "    # eindeutiger Dateiname\n",
    "    filename = os.path.join(streaming_input_volume, f\"event_{i:06d}.json\")\n",
    "    \n",
    "    # als JSON schreiben\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(json.dumps(row_dict))\n",
    "    \n",
    "    print(f\"Wrote event {i+1}\")\n",
    "    time.sleep(3)  # Pause, um „Streaming“ zu imitieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9d6eb6-63c0-4c2d-a683-a9c5b8d709f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "streaming_input_volume = \"/Volumes/workspace/default/taxi_volume/\"\n",
    "\n",
    "# Assume you already have taxi_df\n",
    "rows_per_batch = 100\n",
    "gewichte = 1/rows_per_batch\n",
    "\n",
    "# Split DF into batches\n",
    "batches = df_taxi.randomSplit([gewichte]*rows_per_batch, seed=42)  # 500 mini-batches\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    write_mode = \"overwrite\" if i == 0 else \"append\"\n",
    "    batch.write.mode(write_mode).format(\"delta\").saveAsTable(f\"{streaming_input_volume}/taxi_parquet\")\n",
    "    print(f\"Wrote batch {i+1}\")\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modul 5 - Streaming Input",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
