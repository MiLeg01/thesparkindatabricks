{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c67079a6-92bc-456f-bf6b-1c5f9f018c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Modul 5 - Part 1: Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "658ed026-7e8f-4b61-af9b-f567e6b16961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Inhaltsverzeichnis tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d7cdaa-a73b-4357-855a-4e8cc530e52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.1. Setup und Dataset laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00189ad0-33fe-4783-b405-b9384037bc5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBFS Pfad\n",
    "DATA_PATH = \"workspace.default.yellow_tripdata_2025_01\" # \"/FileStore/tables/yellow_tripdata_2025_01-1.parquet\"\n",
    "LOOKUP_PATH = \"workspace.default.df_lookup\"\n",
    "\n",
    "# DataFrame laden\n",
    "df_taxi = spark.read.table(DATA_PATH)\n",
    "df_lookup = spark.read.table(LOOKUP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "996092e0-33ac-4df9-9d41-d8eefe3092b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.2. Streaming Quelle erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37977f15-be19-4e68-aabf-5669153733b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE IF EXISTS workspace.output.taxi_full\");\n",
    "#spark.sql(\"DROP TABLE IF EXISTS workspace.output.yellow_tripdata_2025_01\");\n",
    "\n",
    "#spark.sql(\"DROP SCHEMA IF EXISTS workspace.output\");\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.streaming_input\");\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.streaming_output\");\n",
    "#spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.streaming_checkpoint\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76102b38-a555-4676-995b-8fce76e141e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935a3c1c-635b-45d2-967a-e9ce4f96df48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "input_path = \"workspace.streaming_input.inputtable\"\n",
    "output_path = \"workspace.streaming_output.outputtable\"\n",
    "#streaming_checkpoint = \"workspace.streaming_checkpoint.checkpointtable\"\n",
    "\n",
    "# Assume you already have taxi_df\n",
    "rows_per_batch = 100\n",
    "gewichte = 1/rows_per_batch\n",
    "\n",
    "# Split DF into batches\n",
    "batches = df_taxi.randomSplit([gewichte]*rows_per_batch, seed=42)  # 500 mini-batches\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    write_mode = \"overwrite\" if i == 0 else \"append\"\n",
    "    batch.write.mode(write_mode).format(\"delta\").saveAsTable(input_path)\n",
    "    print(f\"Wrote batch {i+1}\")\n",
    "    time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eee9d94-b3f7-43be-84e6-bdac939f6c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.3. Stream einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68ba196-298b-4757-9b86-6c5c0391698a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = (\n",
    "    spark.readStream\n",
    "         .table(input_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe6c54d-4a04-430c-aca8-a1b2aa743a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.4. Stream verarbeiten durch Logik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05779066-ae56-4b2c-961b-826628d16c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "agg_df = (\n",
    "    streaming_df\n",
    "        .groupBy(\"passenger_count\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"trip_count\"),\n",
    "            avg(\"fare_amount\").alias(\"avg_fare\")\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4da97b09-7f74-467d-9c3a-4c947638671a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.5. Stream schreiben in Zieltabelle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c43f006-f781-4289-98ab-d805a5b2fb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/Volumes/workspace/streaming_output/checkpoint\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a04bbe-e767-46d3-9110-513e33e5d8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume = \"/Volumes/workspace/streaming_output/checkpoint\"  # just the path\n",
    "\n",
    "query = (\n",
    "    agg_df.writeStream\n",
    "         .outputMode(\"complete\")              # replace results on each trigger\n",
    "         .option(\"checkpointLocation\", volume)\n",
    "         .format(\"delta\")\n",
    "         .trigger(availableNow=True)\n",
    "         .table(output_path)                 # writes back into catalog\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9220078a-36bf-4adf-b355-0a08f2af078c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.streaming_output.outputtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6fc35f-5257-41c4-9448-d89e4a079c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.6 Delta Live Tables"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4934191715752017,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Modul 5 - Part 1: Structured Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
